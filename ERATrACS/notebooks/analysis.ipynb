{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0000b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232d569f",
   "metadata": {},
   "source": [
    "# IBTrACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75833360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_basin(b):\n",
    "    if isinstance(b, bytes):\n",
    "        return b.decode(\"utf-8\")\n",
    "    if isinstance(b, str) and b.startswith(\"b'\"):\n",
    "        return b[2:-1]\n",
    "    return b\n",
    "\n",
    "def load_IBTrACS(path, years):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"time_stamp\"] = pd.to_datetime(df[\"time_stamp\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time_stamp\"])\n",
    "    df[\"basin\"] = df[\"basin\"].apply(clean_basin)\n",
    "    df = df[\n",
    "        (df[\"time_stamp\"].dt.year >= years[0]) &\n",
    "        (df[\"time_stamp\"].dt.year <= years[1])\n",
    "    ]\n",
    "    print(\n",
    "        \"IBTrACS:\",\n",
    "        df[\"time_stamp\"].dt.year.min(), \"→\", df[\"time_stamp\"].dt.year.max(),\n",
    "        \"| obs:\", len(df),\n",
    "        \"| cyclones:\", df[\"sid\"].nunique()\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = Path(\"../../data/processed\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_ibtracs = PROCESSED_DIR / \"other/ibtracs_usa_20251216.csv\"\n",
    "\n",
    "df_ibtracs = load_IBTrACS(path_ibtracs, years=[2022, 2024])\n",
    "\n",
    "df_ibtracs[\"time_stamp\"] = pd.to_datetime(df_ibtracs[\"time_stamp\"], errors=\"coerce\")\n",
    "df_ibtracs = df_ibtracs.dropna(subset=[\"time_stamp\"])\n",
    "\n",
    "years = sorted(df_ibtracs[\"time_stamp\"].dt.year.unique())\n",
    "\n",
    "for year in years:\n",
    "    df_y = df_ibtracs[df_ibtracs[\"time_stamp\"].dt.year == year]\n",
    "\n",
    "    counts = (\n",
    "        df_y.groupby(\"basin\")\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "    counts.plot(\n",
    "        kind=\"bar\",\n",
    "        width=0.7\n",
    "    )\n",
    "\n",
    "    plt.title(f\"IBTrACS obs per basin – {year}\", fontsize=10)\n",
    "    plt.ylabel(\"Count\", fontsize=9)\n",
    "    plt.xlabel(\"Basin\", fontsize=9)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3005e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_counts = (\n",
    "    df_ibtracs[\"time_stamp\"]\n",
    "    .dt.hour\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "hour_counts.plot(\n",
    "    kind=\"bar\",\n",
    "    width=0.7\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Hour (UTC)\", fontsize=9)\n",
    "plt.ylabel(\"Count\", fontsize=9)\n",
    "plt.title(\"Observation hour distribution (IBTrACS)\", fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=0, fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = sorted(df_ibtracs[\"basin\"].unique())\n",
    "\n",
    "for basin in basins:\n",
    "    df_b = df_ibtracs[df_ibtracs[\"basin\"] == basin]\n",
    "\n",
    "    month_counts = (\n",
    "        df_b[\"time_stamp\"]\n",
    "        .dt.month\n",
    "        .value_counts()\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "\n",
    "    month_counts.plot(\n",
    "        kind=\"bar\",\n",
    "        width=0.7\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Monthly number of Observations Basin {basin} (IBTrACS)\", fontsize=10)\n",
    "    plt.xlabel(\"Month\", fontsize=9)\n",
    "    plt.ylabel(\"Obs\", fontsize=9)\n",
    "\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre total de lignes\n",
    "n = len(df_ibtracs)\n",
    "\n",
    "# Taux de NaN par colonne\n",
    "nan_summary = (\n",
    "    df_ibtracs.isna()\n",
    "      .mean()\n",
    "      .sort_values(ascending=False)\n",
    "      .to_frame(\"nan_ratio\")\n",
    ")\n",
    "\n",
    "nan_summary[\"nan_count\"] = df_ibtracs.isna().sum()\n",
    "nan_summary[\"nan_ratio_pct\"] = 100 * nan_summary[\"nan_ratio\"]\n",
    "\n",
    "nan_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = df_ibtracs.isna().sum().sort_values(ascending=False)\n",
    "missing_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd170b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ibtracs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_ibtracs_single_cyclone_points(df, sid):\n",
    "    df_s = (\n",
    "        df[df[\"sid\"] == sid]\n",
    "        .sort_values(\"time_stamp\")\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    if df_s.empty:\n",
    "        print(\"No data for this cyclone\")\n",
    "        return\n",
    "\n",
    "    lat_min, lat_max = df_s[\"lat\"].min(), df_s[\"lat\"].max()\n",
    "    lon_min, lon_max = df_s[\"lon\"].min(), df_s[\"lon\"].max()\n",
    "\n",
    "    p = df_s.iloc[-1]  # point annoté\n",
    "\n",
    "    fig = px.scatter_geo(\n",
    "        df_s,\n",
    "        lat=\"lat\",\n",
    "        lon=\"lon\",\n",
    "        projection=\"natural earth\",\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=7),\n",
    "        hovertemplate=\n",
    "        \"<b>IBTrACS observation</b><br><br>\"\n",
    "        \"<b>Date:</b> %{customdata[0]}<br>\"\n",
    "        \"<b>Latitude:</b> %{lat:.2f}°<br>\"\n",
    "        \"<b>Longitude:</b> %{lon:.2f}°<br><br>\"\n",
    "        \"<b>Intensity</b><br>\"\n",
    "        \"Wind: %{customdata[1]} kt<br>\"\n",
    "        \"Pressure: %{customdata[2]} hPa<br><br>\"\n",
    "        \"<b>Dynamics</b><br>\"\n",
    "        \"Storm speed: %{customdata[3]} m/s<br>\"\n",
    "        \"Storm direction: %{customdata[4]}°\"\n",
    "        \"<extra></extra>\",\n",
    "        customdata=df_s[[\n",
    "            \"time_stamp\",\n",
    "            \"wind\",\n",
    "            \"pressure\",\n",
    "            \"storm_speed\",\n",
    "            \"storm_dir\"\n",
    "        ]].values\n",
    "    )\n",
    "\n",
    "    # ✅ Annotation correcte sur carte\n",
    "    fig.add_annotation(\n",
    "        x=p[\"lon\"],\n",
    "        y=p[\"lat\"],\n",
    "        xref=\"geo\",\n",
    "        yref=\"geo\",\n",
    "        text=(\n",
    "            \"<b>IBTrACS point</b><br>\"\n",
    "            f\"Date: {p['time_stamp']}<br>\"\n",
    "            f\"Wind: {p['wind']} kt<br>\"\n",
    "            f\"Pressure: {p['pressure']} hPa\"\n",
    "        ),\n",
    "        showarrow=True,\n",
    "        arrowhead=2,\n",
    "        ax=40,\n",
    "        ay=-40,\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "        font=dict(size=11),\n",
    "    )\n",
    "\n",
    "    fig.update_geos(\n",
    "        lataxis_range=[lat_min - 2, lat_max + 2],\n",
    "        lonaxis_range=[lon_min - 2, lon_max + 2],\n",
    "        showcountries=True,\n",
    "        showcoastlines=True,\n",
    "        coastlinewidth=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"IBTrACS – Cyclone observations<br><sup>Each point is a spatio-temporal record</sup>\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        showlegend=False,\n",
    "        margin=dict(l=0, r=0, t=60, b=0),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_ibtracs_single_cyclone_points(\n",
    "    df_ibtracs,\n",
    "    sid=\"b'2024181N09320'\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841cfc6",
   "metadata": {},
   "source": [
    "# Bouding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime \n",
    "\n",
    "BASIN_BBOX = {\n",
    "    \"NA\": [60, -100, 0, -10],\n",
    "    \"EP\": [40, -160, 0, -80],\n",
    "    \"WP\": [50, 100, 0, 180],\n",
    "    \"NI\": [30, 40, 0, 100],\n",
    "    \"SI\": [0, 20, -40, 100],\n",
    "    \"SP\": [0, 160, -40, -120],\n",
    "}\n",
    "\n",
    "BASIN_COLORS = {\n",
    "    \"NA\": \"#1f77b4\",  \n",
    "    \"EP\": \"#ff7f0e\",  \n",
    "    \"WP\": \"#2ca02c\",  \n",
    "    \"NI\": \"#d62728\",  \n",
    "    \"SI\": \"#9467bd\",  \n",
    "    \"SP\": \"#8c564b\",  \n",
    "    \"SA\": \"#e377c2\",  \n",
    "}\n",
    "\n",
    "\n",
    "def plot_basin_bounding_boxes_with_template(basin_bbox_dict, basin_colors):\n",
    "    \"\"\"\n",
    "    Plot cyclone basin bounding boxes on a world map.\n",
    "    Correctly handles basins crossing the anti-meridian (±180°).\n",
    "    \"\"\"\n",
    "\n",
    "    # Dummy dataframe just to initialize the geo figure\n",
    "    df_dummy = pd.DataFrame({\n",
    "        \"lon\": [0],\n",
    "        \"lat\": [0],\n",
    "    })\n",
    "\n",
    "    fig = px.line_geo(\n",
    "        df_dummy,\n",
    "        lon=\"lon\",\n",
    "        lat=\"lat\",\n",
    "        projection=\"natural earth\",\n",
    "        title=\"Cyclone basin bounding boxes\",\n",
    "    )\n",
    "\n",
    "    # Remove dummy trace\n",
    "    fig.data = []\n",
    "\n",
    "    for basin, bbox in basin_bbox_dict.items():\n",
    "        north, west, south, east = bbox\n",
    "        color = basin_colors.get(basin, \"black\")\n",
    "\n",
    "        # --- Handle anti-meridian crossing ---\n",
    "        if west <= east:\n",
    "            # Normal case\n",
    "            lon_segments = [(west, east)]\n",
    "        else:\n",
    "            # Anti-meridian case (e.g. SP)\n",
    "            lon_segments = [\n",
    "                (west, 180),\n",
    "                (-180, east),\n",
    "            ]\n",
    "\n",
    "        for i, (w, e) in enumerate(lon_segments):\n",
    "            lats = [south, north, north, south, south]\n",
    "            lons = [w, w, e, e, w]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scattergeo(\n",
    "                    lon=lons,\n",
    "                    lat=lats,\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(width=2, color=color),\n",
    "                    name=basin,\n",
    "                    showlegend=(i == 0),  # avoid legend duplicates\n",
    "                    hoverinfo=\"text\",\n",
    "                    text=(\n",
    "                        f\"Basin {basin}<br>\"\n",
    "                        f\"N: {north}°<br>\"\n",
    "                        f\"S: {south}°<br>\"\n",
    "                        f\"W: {west}°<br>\"\n",
    "                        f\"E: {east}°\"\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # --- Basin label ---\n",
    "        if west <= east:\n",
    "            label_lon = (west + east) / 2\n",
    "        else:\n",
    "            # Place label safely inside visible region\n",
    "            label_lon = 170\n",
    "\n",
    "        label_lat = (north + south) / 2\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scattergeo(\n",
    "                lon=[label_lon],\n",
    "                lat=[label_lat],\n",
    "                mode=\"text\",\n",
    "                text=[basin],\n",
    "                textfont=dict(color=color, size=12),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend_title_text=\"Basin\",\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "        hovermode=\"closest\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_basin_bounding_boxes_with_template(BASIN_BBOX, BASIN_COLORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be728b8",
   "metadata": {},
   "source": [
    "# ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PATH = \"../../data/processed/ibtracs_era5_20251218_0052.csv\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(PATH, parse_dates=[\"Timestamp\"], keep_default_na=False)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Years:\", sorted(df[\"Timestamp\"].dt.year.unique()))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "METEO_VARS = [\n",
    "    \"ERA5_Temp_2m_Kelvin\",\n",
    "    \"ERA5_Pressure_MSL_hPa\",\n",
    "    \"ERA5_Wind_U_Component\",\n",
    "    \"ERA5_Wind_V_Component\",\n",
    "]\n",
    "\n",
    "def summarize_year(df, year):\n",
    "    year = int(year)\n",
    "    df_y = df[df[\"Timestamp\"].dt.year == year]\n",
    "\n",
    "    if df_y.empty:\n",
    "        print(f\"\\n===== SUMMARY {year} =====\")\n",
    "        print(\"No data\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n===== SUMMARY {year} =====\")\n",
    "    print(\"Observations:\", len(df_y))\n",
    "    print(\"Cyclones:\", df_y[\"Storm_ID\"].nunique())\n",
    "    print(\n",
    "        \"Period:\",\n",
    "        df_y[\"Timestamp\"].min(),\n",
    "        \"→\",\n",
    "        df_y[\"Timestamp\"].max(),\n",
    "    )\n",
    "\n",
    "    print(\"\\nObservations per cyclone:\")\n",
    "    print(df_y.groupby(\"Storm_ID\").size().describe())\n",
    "\n",
    "    print(\"\\nMeteorological variables:\")\n",
    "    display(df_y[METEO_VARS].describe())\n",
    "\n",
    "for year in sorted(df[\"Timestamp\"].dt.year.unique()):\n",
    "    summarize_year(df, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ec874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nans_year(df, year):\n",
    "    df_y = df[df[\"Timestamp\"].dt.year == year]\n",
    "\n",
    "    print(f\"\\nNaN check {year}\")\n",
    "    if df_y.empty:\n",
    "        print(\"No data\")\n",
    "        return\n",
    "\n",
    "    print(df_y[METEO_VARS].isna().mean())\n",
    "\n",
    "\n",
    "for year in sorted(df[\"Timestamp\"].dt.year.unique()):\n",
    "    check_nans_year(df, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d134908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cyclone_tracks_year(df, year):\n",
    "    df_y = (\n",
    "        df[df[\"Timestamp\"].dt.year == year]\n",
    "        .sort_values(\"Timestamp\")\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    if df_y.empty:\n",
    "        print(f\"No data for {year}\")\n",
    "        return\n",
    "\n",
    "    fig = px.line_geo(\n",
    "        df_y,\n",
    "        lon=\"Longitude\",\n",
    "        lat=\"Latitude\",\n",
    "        color=\"Storm_ID\",\n",
    "        hover_name=\"Storm_Name\",\n",
    "        projection=\"natural earth\",\n",
    "        title=f\"Cyclone trajectories – {year}\",\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        line=dict(width=2),\n",
    "        hovertemplate=\n",
    "        \"<b>Cyclone:</b> %{customdata[0]}<br>\"\n",
    "        \"<b>Date:</b> %{customdata[1]}<br>\"\n",
    "        \"<b>Lat:</b> %{lat:.2f}°<br>\"\n",
    "        \"<b>Lon:</b> %{lon:.2f}°<br><br>\"\n",
    "        \"<b>IBTrACS</b><br>\"\n",
    "        \"Wind: %{customdata[2]} kt<br>\"\n",
    "        \"Pressure: %{customdata[3]} hPa<br><br>\"\n",
    "        \"<b>ERA5</b><br>\"\n",
    "        \"2m Temp: %{customdata[4]:.1f} K<br>\"\n",
    "        \"MSLP: %{customdata[5]:.1f} hPa<br>\"\n",
    "        \"U10: %{customdata[6]:.1f} m/s<br>\"\n",
    "        \"V10: %{customdata[7]:.1f} m/s\"\n",
    "        \"<extra></extra>\",\n",
    "        customdata=df_y[[\n",
    "            \"Storm_Name\",\n",
    "            \"Timestamp\",\n",
    "            \"Observed_Wind_Max_Knots\",\n",
    "            \"Observed_Pressure_Min_mb\",\n",
    "            \"ERA5_Temp_2m_Kelvin\",\n",
    "            \"ERA5_Pressure_MSL_hPa\",\n",
    "            \"ERA5_Wind_U_Component\",\n",
    "            \"ERA5_Wind_V_Component\",\n",
    "        ]].values\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend_title_text=\"Storm ID\",\n",
    "        margin=dict(l=0, r=0, t=40, b=0),\n",
    "        hovermode=\"closest\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plot_cyclone_tracks_year(df, 2022)\n",
    "plot_cyclone_tracks_year(df, 2023)\n",
    "plot_cyclone_tracks_year(df, 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ibtracs_cyclones_year(df_ibtracs, year): \n",
    "    df_y = df_ibtracs[df_ibtracs[\"time_stamp\"].dt.year == year] \n",
    "    return df_y[\"sid\"].nunique()\n",
    "\n",
    "def count_era5_cyclones_year(df_era5, year):\n",
    "    df_y = df_era5[df_era5[\"Timestamp\"].dt.year == year]\n",
    "    return df_y[\"Storm_ID\"].nunique()\n",
    "\n",
    "def count_ibtracs_obs_year(df_ibtracs, year): \n",
    "    df_y = df_ibtracs[df_ibtracs[\"time_stamp\"].dt.year == year] \n",
    "    return len(df_y)\n",
    "\n",
    "def count_era5_obs_year(df_era5, year):\n",
    "    df_y = df_era5[df_era5[\"Timestamp\"].dt.year == year]\n",
    "    return len(df_y)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for year in years:\n",
    "    n_cyc_ib = count_ibtracs_cyclones_year(df_ibtracs, year)\n",
    "    n_cyc_era = count_era5_cyclones_year(df, year)\n",
    "\n",
    "    n_obs_ib = count_ibtracs_obs_year(df_ibtracs, year)\n",
    "    n_obs_era = count_era5_obs_year(df, year)\n",
    "\n",
    "    rows.append({\n",
    "        \"Year\": year,\n",
    "        \"Cyclones_IBTrACS\": n_cyc_ib,\n",
    "        \"Cyclones_with_ERA5\": n_cyc_era,\n",
    "        \"Cyclone_Coverage_Ratio\": (\n",
    "            n_cyc_era / n_cyc_ib if n_cyc_ib > 0 else np.nan\n",
    "        ),\n",
    "        \"Observations_IBTrACS\": n_obs_ib,\n",
    "        \"Observations_with_ERA5\": n_obs_era,\n",
    "        \"Observation_Coverage_Ratio\": (\n",
    "            n_obs_era / n_obs_ib if n_obs_ib > 0 else np.nan\n",
    "        ),\n",
    "    })\n",
    "\n",
    "df_coverage_full = pd.DataFrame(rows)\n",
    "df_coverage_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ecd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(df[\"ERA5_Position_Error_km\"], bins=10, range=(0, 50))\n",
    "plt.xlabel(\"ERA5 spatial error (km)\")\n",
    "plt.ylabel(\"Number of observations\")\n",
    "plt.title(\"ERA5 spatial error (50 km)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4dff53",
   "metadata": {},
   "source": [
    "Un pas de grille ERA5 de 0.25° à 0.5° correspond à une résolution spatiale de l’ordre de 20 à 50 km selon la latitude, ce qui rend les erreurs spatiales observées (médiane ≈ 20 km, maximum ≈ 30 km) parfaitement cohérentes avec un échantillonnage nearest-neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_overlaps_final_dataset(\n",
    "    df,\n",
    "    key_cols=(\"Storm_ID\", \"Timestamp\"),\n",
    "    strict_cols=(\"Storm_ID\", \"Timestamp\", \"Latitude\", \"Longitude\"),\n",
    "    show_examples=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlap = mêmes clés (Storm_ID, Timestamp) présentes plusieurs fois.\n",
    "    - strict_cols permet de voir si c'est un vrai doublon (mêmes coords) ou un conflit (coords différentes).\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "\n",
    "    # --- 1) Doublons sur la clé principale (Storm_ID, Timestamp) ---\n",
    "    dup_mask = df2.duplicated(list(key_cols), keep=False)\n",
    "    df_dup = df2[dup_mask].sort_values(list(key_cols))\n",
    "\n",
    "    print(\"=== Overlap check ===\")\n",
    "    print(\"Rows:\", len(df2))\n",
    "    print(\"Unique keys:\", df2[list(key_cols)].drop_duplicates().shape[0])\n",
    "    print(\"Duplicate-key rows:\", len(df_dup))\n",
    "    print(\"Duplicate keys count:\", df2[list(key_cols)].duplicated().sum())\n",
    "\n",
    "    if df_dup.empty:\n",
    "        print(\"\\nAucun overlap sur la clé\", key_cols)\n",
    "        return {\n",
    "            \"duplicate_key_rows\": 0,\n",
    "            \"duplicate_keys\": 0,\n",
    "            \"strict_duplicate_rows\": 0,\n",
    "            \"conflict_keys\": 0,\n",
    "        }\n",
    "\n",
    "    # --- 2) Séparer \"vrais doublons\" vs \"conflits\" ---\n",
    "    # Vrai doublon = mêmes strict_cols répétés\n",
    "    strict_dup_mask = df2.duplicated(list(strict_cols), keep=False)\n",
    "    df_strict_dup = df2[strict_dup_mask].sort_values(list(strict_cols))\n",
    "\n",
    "    # Conflit = même (Storm_ID, Timestamp) mais pas exactement mêmes strict_cols\n",
    "    # => on regarde le nb de lignes distinctes par clé après réduction sur strict_cols\n",
    "    grp = df_dup.groupby(list(key_cols))\n",
    "    conflict_keys = []\n",
    "    for k, g in grp:\n",
    "        if g[list(strict_cols)].drop_duplicates().shape[0] > 1:\n",
    "            conflict_keys.append(k)\n",
    "\n",
    "    print(\"\\n--- Details ---\")\n",
    "    print(\"Strict-duplicate rows (exact same Storm_ID+Timestamp+Lat+Lon):\", len(df_strict_dup))\n",
    "    print(\"Conflict keys (same Storm_ID+Timestamp but different Lat/Lon):\", len(conflict_keys))\n",
    "\n",
    "    # --- 3) Afficher exemples ---\n",
    "    print(\"\\n--- Example duplicate-key rows ---\")\n",
    "    display(df_dup.head(show_examples))\n",
    "\n",
    "    if conflict_keys:\n",
    "        print(\"\\n--- Example conflict key ---\")\n",
    "        k0 = conflict_keys[0]\n",
    "        display(df_dup[(df_dup[\"Storm_ID\"] == k0[0]) & (df_dup[\"Timestamp\"] == k0[1])])\n",
    "\n",
    "    return {\n",
    "        \"duplicate_key_rows\": int(len(df_dup)),\n",
    "        \"duplicate_keys\": int(df2[list(key_cols)].duplicated().sum()),\n",
    "        \"strict_duplicate_rows\": int(len(df_strict_dup)),\n",
    "        \"conflict_keys\": int(len(conflict_keys)),\n",
    "    }\n",
    "\n",
    "# Usage:\n",
    "stats = check_overlaps_final_dataset(df)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4220e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8032b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
